---
title: "P8105_hw3_mc5296"
output: github_document
date: "2022-10-07"
---

```{r}
library(tidyverse)
library(ggplot2)
library(patchwork)
library(ggridges)
```

# Problem 1

## Import Instacart

```{r}
library(p8105.datasets) # Already installed datasets for the homework
data("instacart")
dim(instacart)
head(instacart)
```

The datasets has 1384617 rows(which means there are 1384617 orders), and 15 variables. The key variables is order_dow, order_hour, product name, products' aisle and department.

## EDA

```{r}
instacart%>%
  count(aisle, name = "n_aisle") %>%
  arrange(desc(n_aisle)) # count the number of each of the aisle and arrange them in a decreasing order.
```

There are 134 aisles. The fresh vegetables and fresh fruits are the item most ordered.

```{r}
instacart %>%
  count(aisle, name = "n_aisle") %>%
  arrange(desc(n_aisle)) %>%
  filter(n_aisle > 10000) %>% # select the aisles with more than 10000 items
ggplot(aes(x = aisle, y = n_aisle,width = 1)) +
  geom_bar(stat = "identity") + 
  theme(axis.title =  element_text(size=10,face = "bold"), # set the size of the xlabel
        axis.text.x = element_text(angle=70,  # rotate the xlabel
                                     hjust = 1)) # adjust the position of the xlabel text
```

```{r}
filtered_table <- instacart %>%
  select(aisle, product_name) %>%
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits"))%>% # select the row and column we need
  group_by(aisle,product_name) %>%
  summarize(n_product = n())  
filtered_table %>%
  group_by(aisle)%>%
  summarize(number_most_popular_items = max(n_product)) %>% # find the number of most popular item of each aisle
  left_join(filtered_table, c("number_most_popular_items" = "n_product","aisle" = "aisle")) #find the correspond items to the number of most popular item
```

As is shown in the table , the most popular item of baking ingredients is light brown sugar, the most popular dog food care item is snack sticks chicken & rice recipe dog treats

## Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers

```{r}
instacart %>%
  select(order_hour_of_day, order_dow, product_name) %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream"))%>% # we only need these two products
  group_by(product_name,order_dow)%>%
  summarise(mean_hour_of_the_day = mean(order_hour_of_day)) %>% # calculate the mean purchase hour for each pf the product pf each day in the week
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour_of_the_day
) %>%  # make the table more readable
  rename(Mean_hour_Sun = "0", Mean_hour_Mon = "1", Mean_hour_Tue="2", Mean_hour_Wed = "3", Mean_hour_Thu = "4", Mean_hour_Fri = "5", Mean_hour_Sat = "6")%>% 
  knitr::kable(digits = 1)
```

# P2

## Import data

```{r}
accelerometer <- read.csv("data/accel_data.csv")
dim(accelerometer)
```

## Tidy and Wrangle the data

```{r}
accelerometer %>%
  janitor::clean_names() %>%
  pivot_longer(activity_1:activity_1440,
               names_to = "minute_of_day",
               names_prefix = "activity.",
               values_to = "activity_count") %>% # convert the length and width to make the dataframe looks better.
  mutate(
    weekday_VS_weekend = ifelse(day %in% c("Saturday","Sunday"), "Weekend", "weekday")
  ) # seperate the  weekday and weekend
```

The datasets has 50400 rows which records the data of activity of every minute in 35days. It has the id of the week, day , day in a week, minute of the day and the activity in the minute. \## Aggregate across minutes to create a total activity variable for each day, and create a table showing these totals.

```{r}
accelerometer %>%
  janitor::clean_names() %>%
  pivot_longer(activity_1:activity_1440,
               names_to = "minute_of_day",
               names_prefix = "activity.",
               values_to = "activity_count") %>%
               group_by(week,day)%>%
               summarize(total_activity_day = sum(activity_count)) %>%
  arrange(total_activity_day)
```

I arranged the total activity of each day by descending. As is shown in the table, there is no obvious trend. But there are two days that are abnormal,which are the Saturday of the 4th and 5th week. The activity count of every minute is 1. 

## Make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week.

```{r}
accelerometer %>%
  janitor::clean_names() %>%
  pivot_longer(activity_1:activity_1440,
               names_to = "minute_of_day",
               names_prefix = "activity.",
               values_to = "activity_count") %>%
  ggplot(aes(x = as.numeric(minute_of_day), y = activity_count,color = day)) +  #"minute_of_day" is a character vector, so we need to convert it to numeric vector.
  labs(title = "Activity account in each day") +
  geom_point() + geom_line() + labs(x = "minute of day")
```

As is shown in the plot, the activity count is mostly low during night and high during the day. It is especially high in the morning and evening. 

# P3 

## import the data

```{r}
library(p8105.datasets)
data("ny_noaa")
head(ny_noaa)
dim(ny_noaa)
summary(ny_noaa)
```
The dataset has 2595176 rows and  7 variables. "Id" is the key for every distinct station. "date" is the recording date,"prcp is precipation and snow is snowfall  of that day. "Snwd" is snow depth. And we also have "tmax" and "tmin", which represents the max and min tempature of that day. These data are recorded by each distinct station. We have thousands missing data in ‚Äùprcp", "snwd","snow", "tmax","tmin". In order to calculate the mean of these variables. We had to remove the missing values.
## Data cleaning

```{r}
cleaned_noaa <- ny_noaa %>%
  separate(date, into = c("year","month","day"),sep = "-" ) %>%
  mutate(
    prcp = prcp/10,
    tmax = as.numeric(tmax)/10,
    tmin = as.numeric(tmin)/10  # make the units resonable
  ) 
summary(cleaned_noaa)
```

The most observation for snowfall is 0, for it is normal there is no snow in most of a year and we expect to see snow only in winter.

## Make a two-panel plot showing the average max temperature in January and in July in each station across years

```{r}
cleaned_noaa %>%
  filter(month %in% c("01","07")) %>% 
  group_by(id,year,month)%>%
  summarize(
    mean_tmax = mean(tmax,na.rm = TRUE)  # remove the missing values, otherwise we cannot calculate the mean.
  ) %>%
ggplot(aes(x= as.numeric(year), y = mean_tmax, color = id))+
  geom_point() +
  geom_line() +
  facet_grid(.~ month)+
  labs(title = "Mean max temperature of January and July from 1981 to 2010",
       x = "Year",
       y = "mean max temperature ") +
   theme(
     legend.position = "none",
     axis.title =  element_text(size=9,face = "bold"), # set the size of the xlabel
        axis.text.x = element_text(angle=70,  # rotate the xlabel
                                     hjust = 1)) # adjust the position of the xlabel text
```
Here I used line to show the trends of mean max temperature of January and JUly across the years. The range of mean max temperature in January is in a range of -10 degress to  10 degrees.In  July, the range is  20  to 35. Both of them fluctuated with the year.
I can see some outliers, For example, in July's plot. There is a point below 15, which is abnormal in July. Similar, there is a very low point in January, 1982
```{r}
p1 <- cleaned_noaa %>%
  filter(!is.na(tmin), !is.na(tmax))%>%
  ggplot(aes(x = tmin, y = tmax))+
  geom_hex()+
  theme(legend.position = "left")

p2<- cleaned_noaa %>%
  filter(snow>0 & snow<100)%>%
  ggplot(aes(x = snow,y = year)) +
  geom_density_ridges(alpha = 0.5) +
  labs(x = "Snowfall(mm)") +
  theme(axis.title =  element_text(size=9,face = "bold"), # set the size of the xlabel
        axis.text.x = element_text(angle=70,  # rotate the xlabel
                                     hjust = 1)) # adjust the position of the xlabel text

p1 + p2
```
